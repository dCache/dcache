<configuration>

  <!-- dCache logs to a hierarchy of log targets or loggers. The root
       of this hierarchy is called root. All other loggers have a name
       that is typically the class name of the component logging. Most
       of the time the specific name of a logger is not important, as
       one typically only adjusts the log level of the root
       logger. Unless specifically redefined, the log level is
       inherited from the parent logger and ultimately from the root
       logger.

       Appenders are output components that write log messages
       somewhere. In dCache, log files are generated by redirecting
       standard out to the log file. Hence there is a ConsoleAppender
       for logging to stdout. Another appender directs log messages to
       the cell pinboard. The pinboard is accessible through the
       dCache admin shell.

       Please consult the logback manual for information about other
       appenders. There exist appenders for writing directly to log
       files, remote logging, logging through syslog, and to generate
       emails.

       Appenders are attached to loggers at specific points in the
       logger hierarchy, typically the root.

       In logback classic the log level is adjusted directly for a
       logger. This proves insufficient for dCache. Therefore we
       instantiate a CellThresholdFilter. This filter allows the log
       level to be defined for a logger per appender and per
       cell. Thus rather than adjusting the log level of a logger
       globally, one adjusts it for a specific appender and a specific
       cell. The filter can be readjusted for each cell at runtime
       through the dCache admin shell.

       Besides the inheritance in the logger hiearchy, log level
       definitions are inherited between cells: If a cell creates
       another cell, then the log level definitions of the parent cell
       are inherited. This is relevant for doors that create a cell
       instance per connection: The log level can be adjusted for the
       master door cell and the cells created per connection will
       inherit the log levels from the master cell. The root of this
       inheritance hirarchy is defined in the filter definition below
       as threshold-tags. Each tag defines a log level for a
       particular logger. The definition can be limited to a
       particular appender or apply to all appenders.

       Given a particular appender, the algorithm for determining the
       effective log level for a logger l and a cell c is:

       1. If a log level for l and c is defined then that is the effective
          log level.

       2. Otherwise use the log level for l in the lowest ancestor of c
          for which the log level is defined.

       3. If there is no such lowest ancestor for c, then use the
          effective log level for c of the parent logger.

       Don't worry too much about the specific algorithm. You do not
       need to understand it in details to adjust log levels at
       runtime through the dCache admin shell.
  -->

  <appender name="stdout" class="ch.qos.logback.core.ConsoleAppender">
    <encoder>
      <pattern>${dcache.log.format.file}</pattern>
    </encoder>
  </appender>

  <appender name="pinboard" class="dmg.util.PinboardAppender">
    <layout>
      <pattern>${dcache.log.format.pinboard}</pattern>
    </layout>
  </appender>

  <appender name="events" class="ch.qos.logback.core.rolling.RollingFileAppender">
    <file>${dcache.log.dir}/${dcache.domain.name}.events</file>
    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
      <fileNamePattern>${dcache.log.dir}/${dcache.domain.name}.events.%d{yyyy-MM-dd}.gz</fileNamePattern>
      <maxHistory>0</maxHistory>
    </rollingPolicy>
    <encoder>
        <!-- Implements event based logging following the NetLogger format.

             The log format was originally documented as a CEDPS best practice recommendation,
             however CEDPS no longer exists. A more current description of the format can
             be found at https://docs.google.com/document/d/1oeW_l_YgQbR-C_7R2cKl6eYBT5N4WSMbvz0AT6hYDvA

             The NetLogger project can be found at http://netlogger.lbl.gov -->
        <pattern>%m%n</pattern>
    </encoder>
  </appender>

  <appender name="access" class="ch.qos.logback.core.rolling.RollingFileAppender">
    <file>${dcache.log.dir}/${dcache.domain.name}.access</file>
    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
      <fileNamePattern>${dcache.log.dir}/${dcache.domain.name}.access.%d{yyyy-MM-dd}.gz</fileNamePattern>
      <maxHistory>${dcache.log.access.max-history}</maxHistory>
    </rollingPolicy>
    <encoder>
       <!-- Access log for doors. Currently only supported by SRM. Follows
            the NetLogger format as described above.
         -->
      <pattern>%m%n</pattern>
    </encoder>
  </appender>

  <appender name="zookeeper" class="ch.qos.logback.core.rolling.RollingFileAppender">
    <file>${dcache.log.dir}/${dcache.domain.name}.zookeeper</file>
    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
      <fileNamePattern>${dcache.log.dir}/${dcache.domain.name}.zookeeper.%d{yyyy-MM-dd}.gz</fileNamePattern>
      <maxHistory>${dcache.log.zookeeper.max-history}</maxHistory>
    </rollingPolicy>
    <encoder>
       <!-- Log of all ZooKeeper events. -->
      <pattern>%d %m%n</pattern>
    </encoder>
  </appender>

  <!-- The conditional is to avoid an infinite retry of a non-existent connection
       by the socket appender if an alarms service is not running, which,
       for example, is the case when remote logging is left off system-wide. -->
  <if condition='!"${dcache.log.level.remote}".equals("off")'>
    <then>
      <appender name="socket" class="ch.qos.logback.classic.net.SocketAppender">
        <!-- Regardless of the level to which the remote appender is set,
             only messages marked as ALARM are allowed through -->
        <filter class="org.dcache.alarms.logback.AlarmFilter"/>
        <remoteHost>${dcache.log.server.host}</remoteHost>
        <port>${dcache.log.server.port}</port>
        <reconnectionDelay>10000</reconnectionDelay>
      </appender>
      <appender name="remote" class="ch.qos.logback.classic.AsyncAppender">
        <appender-ref ref="socket" />
      </appender>
    </then>
  </if>

  <appender name="resilience" class="ch.qos.logback.core.rolling.RollingFileAppender">
    <file>${dcache.log.dir}/${dcache.domain.name}.resilience</file>
    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
      <fileNamePattern>${dcache.log.dir}/${dcache.domain.name}.resilience.%d{yyyy-MM-dd}.gz</fileNamePattern>
      <maxHistory>${dcache.log.resilience.max-history}</maxHistory>
    </rollingPolicy>
    <encoder>
      <pattern>${dcache.log.format.file}</pattern>
    </encoder>
  </appender>

  <root>
    <appender-ref ref="stdout"/>
    <appender-ref ref="pinboard"/>
    <if condition='!"${dcache.log.level.remote}".equals("off")'>
      <then>
        <appender-ref ref="remote"/>
      </then>
    </if>
  </root>

  <logger name="org.dcache.events" additivity="false">
    <appender-ref ref="events"/>
  </logger>

  <logger name="org.dcache.access" additivity="false">
    <appender-ref ref="access"/>
  </logger>

  <logger name="org.dcache.zookeeper" additivity="false">
    <appender-ref ref="zookeeper"/>
  </logger>

  <logger name="org.dcache.resilience-log" additivity="false">
    <appender-ref ref="resilience"/>
  </logger>

  <!-- Nothing is logged to this logger. Its sole purpose is to list
       all appenders available; this ensures that the appenders are
       available in the dCache admin interface. -->
  <logger name="dummy" level="OFF">
    <appender-ref ref="stdout"/>
    <appender-ref ref="pinboard"/>
    <appender-ref ref="events"/>
    <appender-ref ref="access"/>
    <appender-ref ref="zookeeper"/>
    <if condition='!"${dcache.log.level.remote}".equals("off")'>
      <then>
        <appender-ref ref="remote"/>
      </then>
    </if>
    <appender-ref ref="resilience"/>
  </logger>

  <turboFilter class="dmg.util.logback.CellThresholdFilter">
    <!-- Important: This turboFilter must be instantiated after
                    appenders and loggers have been instantiated. -->
    <onHigherOrEqual>ACCEPT</onHigherOrEqual>
    <onLower>DENY</onLower>
    <threshold>
      <logger>root</logger>
      <level>off</level>
    </threshold>
    <threshold>
      <logger>COM.claymoresystems.ptls.SSLDebug</logger>
      <level>off</level>
    </threshold>
    <threshold>
      <logger>org.glite.security.trustmanager.CRLFileTrustManager</logger>
      <level>error</level>
    </threshold>
    <threshold>
      <logger>org.glite.security.trustmanager.ContextWrapper</logger>
      <level>off</level>
    </threshold>
    <threshold>
      <logger>org.glite.security.trustmanager.axis.AXISSocketFactory</logger>
      <level>off</level>
    </threshold>
    <threshold>
      <logger>org.glite.security.util.DirectoryList</logger>
      <level>off</level>
    </threshold>
    <threshold>
        <!-- Logs all Netty I/O. Very noisy. -->
        <logger>io.netty.handler.logging.LoggingHandler</logger>
        <level>off</level>
    </threshold>
    <threshold>
        <!-- With v3.4, ZK logs a stacktrace when the client
             disconnects unexpectedly.  This is fixed with v3.5 and
             later.  See
             https://issues.apache.org/jira/browse/ZOOKEEPER-2809 -->
        <logger>org.apache.zookeeper.server.NIOServerCnxn</logger>
        <level>error</level>
    </threshold>
    <threshold>
        <!-- ZK currently contains a race-condition that triggers a
             CancelledKeyException on shutdown; if triggered, a
             stack-trace is logged at WARN level. See
             https://issues.apache.org/jira/browse/ZOOKEEPER-2838
             Adjusting log-level as a work-around. -->
        <logger>org.apache.zookeeper.server.NIOServerCnxnFactory</logger>
        <level>error</level>
    </threshold>
    <threshold>
      <logger>liquibase</logger>
      <level>warn</level>
    </threshold>

    <threshold>
      <!--If broker is not available processDisconnection continuously logs "Connection to node {}
      could not be established. Broker may not be available." warning. -->
      <logger>org.apache.kafka.clients.NetworkClient</logger>
      <level>error</level>
    </threshold>

    <threshold>
      <appender>stdout</appender>
      <logger>root</logger>
      <level>${dcache.log.level.file}</level>
    </threshold>

    <if condition='!"${dcache.log.level.remote}".equals("off")'>
      <then>
        <threshold>
          <appender>remote</appender>
          <logger>root</logger>
          <level>${dcache.log.level.remote}</level>
        </threshold>
      </then>
    </if>

    <threshold>
      <appender>pinboard</appender>
      <logger>root</logger>
      <level>${dcache.log.level.pinboard}</level>
    </threshold>

    <threshold>
      <appender>events</appender>
      <logger>org.dcache.events</logger>
      <level>${dcache.log.level.events}</level>
    </threshold>

    <threshold>
      <appender>access</appender>
      <logger>org.dcache.access</logger>
      <level>${dcache.log.level.access}</level>
    </threshold>

    <threshold>
      <appender>zookeeper</appender>
      <logger>org.dcache.zookeeper</logger>
      <level>${dcache.log.level.zookeeper}</level>
    </threshold>

    <threshold>
      <appender>resilience</appender>
      <logger>org.dcache.resilience-log</logger>
      <level>${dcache.log.level.resilience}</level>
    </threshold>

  </turboFilter>
</configuration>
