#  -----------------------------------------------------------------------
#     Default values for qos
#
#     The qos services are responsible for maintaining the disk and tape
#     requirements of a given file.
#  -----------------------------------------------------------------------
@DEFAULTS_HEADER@

#  ---- Cell names of qos services
#
qos.cell.name=qos
qos.engine.cell.name=qos-engine
qos.verifier.cell.name=qos-verifier
qos.scanner.cell.name=qos-scanner
qos.adjuster.cell.name=qos-adjuster

#  ---- Named queues to consume from
#
#       A service can consume messages from named queues. Other services can
#       write messages to such queues. A named queue has an unqualified cell
#       address, that is, an address without a domain name.
#
#       This property contains a comma separated list of named queues to
#       consume from.
#
qos.cell.consume = ${qos.cell.name}
qos.engine.cell.consume = ${qos.engine.cell.name}
qos.verifier.cell.consume = ${qos.verifier.cell.name}
qos.scanner.cell.consume = ${qos.scanner.cell.name}
qos.adjuster.cell.consume = ${qos.adjuster.cell.name}

#  ---- Message topics to subscribe to.
#
qos.cell.subscribe=${qos.cache-location-topic},\
  ${qos.corrupt-file-topic},\
  ${qos.pool-monitor-topic}

qos.engine.cell.subscribe=${qos.cache-location-topic},\
  ${qos.corrupt-file-topic},\
  ${qos.pool-monitor-topic}

qos.verifier.cell.subscribe=${qos.pool-monitor-topic}

qos.scanner.cell.subscribe=${qos.pool-monitor-topic}

# ---- Listens for location updates from PnfsManager.
#
qos.cache-location-topic=CacheLocationTopic

# ---- Listens for checksum scanner or pool reports.  If the corrupt
#      file is a non-unique replica, it tries to handle this by removing
#      the copy and making a new one.
#
qos.corrupt-file-topic=${dcache.corrupt-file.topic}

# ---- Channel on which pool monitor updates are pushed out.
#      Resilience relies on these for current info regarding pools,
#      pool groups, storage units, pool mode/status, pool tags, and pool cost.
#
qos.pool-monitor-topic=${dcache.pool-monitor.topic}

# ---- Publishes transition completed messages on this topic.
#
qos.transition-completed-topic=${dcache.qos.transition-topic}

# ---- Base directory where any qos metadata is stored. This inaccessible file lists and statistics
#      output.
#
qos.home=${dcache.paths.qos}

# ---- Configuration for namespace database connection pool ---------------------------
#
#      The database connection pool reuses connections between successive
#      database operations.  By reusing connections dCache doesn't suffer
#      the overhead of establishing new database connections for each
#      operation.
#
#      The options here determine how qos behaves as the number of concurrent
#      requests fluctuates.
# ---------------------------------------------------------------------------

# ---- The maximum number of concurrent database connections
#
#      The recommended minimum setting is the number of scan threads
#      plus a few more for admin calls.
#
#      Since the scanner service shares the chimera database with pnfsmanager,
#      be sure to adjust the postgresql.conf max connections upwards
#      to accommodate both.  Pnfsmanager runs well with about 100
#      connections.  Adding a separate qos service means the
#      connections should be increased by at least the amount below.
#
qos.db.namespace.connections.max=10

# ---- The minimum number of idle database connections.
#
qos.db.namespace.connections.idle=1

(prefix)qos.db.namespace.hikari-properties = Hikari-specific properties

# ---- Database related settings reserved for internal use.
#
(immutable)qos.db.namespace.host=${chimera.db.host}
(immutable)qos.db.namespace.name=${chimera.db.name}
(immutable)qos.db.namespace.user=${chimera.db.user}
(immutable)qos.db.namespace.password=${chimera.db.password}
(immutable)qos.db.namespace.password.file=${chimera.db.password.file}
(immutable)qos.db.namespace.url=${chimera.db.url}
(immutable)qos.db.namespace.schema.changelog=${chimera.db.schema.changelog}
(immutable)qos.db.namespace.schema.auto=false

# ---- Used with the pool scan query. This is a hint given to the jdbc driver
#      to decrease the number of round-trips to the database on large result
#      sets (by default it is 0, meaning ignored).  Setting this too high
#      may, however, adversely affect performance.
#
qos.db.namespace.fetch-size=1000

# ---- Configuration for verifier database connection pool ---------------------------
#
#      The database connection pool reuses connections between successive
#      database operations.  By reusing connections dCache doesn't suffer
#      the overhead of establishing new database connections for each
#      operation.
#
#      The options here determine how qos behaves as the number of concurrent
#      requests fluctuates.
# ---------------------------------------------------------------------------
qos.db.verifier.connections.max=10

# ---- The minimum number of idle database connections.
#
qos.db.verifier.connections.idle=1

(prefix)qos.db.verifier.hikari-properties = Hikari-specific properties

# ---- Database related settings reserved for internal use.
#
(immutable)qos.db.verifier.name=qos
(immutable)qos.db.verifier.host=${dcache.db.host}
(immutable)qos.db.verifier.user=${dcache.db.user}
(immutable)qos.db.verifier.password=${dcache.db.password}
(immutable)qos.db.verifier.password.file=${dcache.db.password.file}
(immutable)qos.db.verifier.url=jdbc:postgresql://${qos.db.verifier.host}/${qos.db.verifier.name}?targetServerType=master
(immutable)qos.db.verifier.schema.changelog=org/dcache/qos/model/db.changelog-master.xml
(immutable)qos.db.verifier.schema.auto=${dcache.db.schema.auto}

# ---- Used with listing of file operations.
#
qos.db.verifier.fetch-size=1000

# ---- Replace with org.dcache.chimera.namespace.ChimeraEnstoreStorageInfoExtractor
#      if you are running an enstore HSM backend.
#
qos.plugins.storage-info-extractor=${dcache.plugins.storage-info-extractor}

# ---- Thread queues --------------------------------------------------------------
#
#      There are different thread queues associated with each of the qos services.
#
#      In general, each (remote) service has an executor for handling the
#      processing of incoming messages.  The thread pools for these
#      are labeled 'submit-threads.'  In the case of the verifier,
#      there is also a bulk submission pool for handling bulk scan requests.
#
#      The verifier, scanner and adjuster in addition also have task thread pools.
# ---------------------------------------------------------------------------------

# ---- Thread queue used during the request for and update to requirements.
#      In the current implementation, each thread makes a call to the namespace.
#
qos.limits.requirements.submit-threads=32

# ---- Thread queue used during the registration of a new operation.
#      Updates will do a verification of the pool locations; cancellation
#      also runs on these threads, but involves simply setting up a filter
#      (the actual cancellation is run on a different thread).
#
qos.limits.verifier.submit-threads=32

# ---- Thread queue used during the registration of a new operation.
#      Like the above, but loops over a list of files.
#
qos.limits.verifier.bulk-threads=8

# ---- Thread queue used when an operation becomes active (to verify
#      the requirements and send a message to the adjuster). Each thread makes
#      a call to the requirements service, then to the pools, and finally,
#      if necessary, to the adjuster service.
#
qos.limits.verifier.task-threads=32

# ---- Thread queue used to handle retirement of the operation.
#      Processes communication and removal from the store.
#
qos.limits.verifier.remove-threads=16

# ---- Thread queue used during the registration of a new adjustment task.
#      Minimal work is done on this thread.
#
qos.limits.adjuster.submit-threads=16

# ---- Thread queue used for tasks.  Note that the longer-running tasks
#      like staging relinquish the thread by waiting.
#
qos.limits.adjuster.task-threads=200

# ---- Thread queue used to handle responses from the verifier.  These
#      involve batched counts, and the amount of update work done on the
#      thread is small.  Should mirror the bulk threads on the verifier.
#
qos.limits.scanner.submit-threads=8

# ---- Thread queue used for scanning the namespace on pool state changes or
#      as part of a periodic check.  Requires a database connection,
#      which it holds onto for the life of the task being executed.
#
#      A note on pool operation throttling:
#
#      A pool scan or processing of a pool status message can generate
#      thousands, even millions, of file tasks.  Allowing too many pool
#      operations to run simultaneously can, aside from the increased
#      pressure on the namespace database, potentially overload the system.
#      Lowering the number of available threads may be necessary
#      if the number of files per pool is on the order of 2 million or
#      greater (or, alternately, one may need to increase the memory of the
#      JVM for the scanner service).
#
qos.limits.scanner.task-threads=5


qos.limits.scanner.pool-op-init-grace-period=5
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.scanner.pool-op-init-grace-period.unit=MINUTES

# ---- This number need not reflect the number of available task threads
#      in the adjuster itself.
#
qos.limits.verifier.max-running-operations=200

# ---- Maximum number of operation entries to retrieve at one time from the
#      database and store in the in-memory cache.
qos.limits.verifier.cache-capacity=10000

# ---- Size of buffer for displaying history of the most
#      recently completed tasks and operations.
#
qos.limits.adjuster.task-history=1000
qos.limits.verifier.operation-history=1000

# ---- Retry management.
#
#      The following property controls the number of
#      times the verifier is allowed to retry failed file-operations.
#      This is on a per-source/target basis, if the error is judged retriable.
#      If there is a non-retriable error, but a different source or target
#      can be selected, the retry count is set back to 0 again.
#
qos.limits.verifier.operation-retries=1

# ---- Retry management.
#
#      The following property controls the number of
#      times the adjuster is allowed to retry failed adjustment tasks.
#
qos.limits.adjuster.task-retries=1

# ---- Operation and task map checking.
#
#      The maximum interval which can pass before a check of waiting/completed
#      operations or tasks is run (for an active system the interval will effectively
#      be shorter, as checks are also done each time a running task terminates).
#
qos.limits.verifier.scan-period=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.verifier.scan-period.unit=MINUTES
qos.limits.adjuster.scan-period=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.adjuster.scan-period.unit=MINUTES

# ---- Pool manager pool info refreshing.
#
#      Information concerning pool cost is considered out of sync after
#      this interval has passed.   This should be somewhat longer than
#      the notification period value(see poolmanager.pool-monitor.update-period and
#      poolmanager.pool-monitor.update-period.unit).
#
qos.limits.pool-info-expiry=3
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.pool-info-expiry.unit=MINUTES

# ---- Pool Status update handling.
#
#      How long to wait between the reception of a pool down update
#      and actually launching a scan operation to check replicas on
#      that pool.  Setting to 0 will trigger the scan immediately.
#
qos.limits.scanner.down-grace-period=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.scanner.down-grace-period.unit=HOURS

# ---- Pool Status update handling.
#
#      How long to wait between the reception of a pool restart update
#      and actually launching a scan operation to check replicas on
#      that pool. Setting to 0 will trigger the scan immediately.
#
qos.limits.scanner.restart-grace-period=6
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.scanner.restart-grace-period.unit=HOURS

# ---- Pool Status update handling.
#
#      How long to allow a pool scan operation which is running to wait to be updated.
#      When this window expires, the scan will be canceled.
#
qos.limits.scanner.pool-op-max-idle-time=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.scanner.pool-op-max-idle-time.unit=HOURS

# ---- Startup
#
#      When an entire dcache installation is brought on line at the same time,
#      pool status may not yet be available from the pool manager.  This
#      property sets an initial delay before pool info initialization
#      begins.  Setting this property to 0 skips the delay.
#
qos.limits.startup-delay=30
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.startup-delay.unit=SECONDS

# ---- Message passing.
#
#      To avoid congestion, particularly in the verifier, messages are sent to the scanner
#      and to the engine in batches.
#
qos.limits.messages.max-batch-size=256
qos.limits.messages.batch-timeout=10
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.messages.batch-timeout.unit=SECONDS

# ---- Reloading.
#      It is possible that the verifier come on line faster than pools being registered, if
#      this is a full system restart.  Not to incur numerous errors, in case there are incomplete
#      operations in the store, we establish a grace period before reload.
#
qos.limits.verifier.reload-grace-period=5
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.verifier.reload-grace-period.unit=MINUTES

# ---- Periodic system scanning
#
#      The following properties control the periodic scanning to check
#      for qos consistency and initiate any adjustments that may be necessary
#      in the case of inconsistent state.   These scans touch all the inodes in the namespace
#      once and only once, in ascending order and according to the specific query.
#
#      The scan period refers to the default amount of time between sweeps to check for timeouts.
#      It is applied to the main thread of both the Pool Operation map and the System Operation map.
#
#      The scan windows refers to the amount of time between scheduled periodic
#      system diagnostic scans. NEARLINE means files which are CUSTODIAL NEARLINE and currently
#      have a cached copy; ONLINE refers to scans of all files with persistent copies, whether
#      or not they are REPLICA or CUSTODIAL. NEARLINE is disabled by default because it
#      can be very time consuming on large namespaces, but it may also be activated
#      using the admin command for occasional diagnostic checks during relatively idle periods.
#
#      The batch size for NEARLINE is lowered to serve as an implicit backgrounding or
#      de-prioritization (since the scan is done in batches, this allows for pre-emption by
#      ONLINE scans if they are running concurrently.
#
qos.limits.scanner.scan-period=3
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.scanner.scan-period.unit=MINUTES
qos.limits.scanner.online-window=24
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.scanner.online-window.unit=HOURS
qos.limits.scanner.enable.nearline-scan=false
qos.limits.scanner.nearline-window=5
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.limits.scanner.nearline-window.unit=DAYS
qos.limits.scanner.online-batch-size=500000
qos.limits.scanner.nearline-batch-size=200000

# ---- Copy/migration target selection.
#
#      Strategy implementation used to select among available/eligible target pools.
#
qos.pool-selection-strategy=org.dcache.pool.migration.ProportionalPoolSelectionStrategy


# ---- Endpoint (cell) settings for contacting pin manager.
#
qos.service.pinmanager=${dcache.service.pinmanager}
qos.service.pinmanager.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.pinmanager.timeout.unit=MINUTES

# ---- Endpoint (cell) settings for contacting pnfs manager.
#
qos.service.pnfsmanager=${dcache.service.pnfsmanager}
qos.service.pnfsmanager.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.pnfsmanager.timeout.unit=MINUTES

# ---- Endpoint (cell) settings for contacting pools (destination is dynamic).
#
qos.service.pool.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.pool.timeout.unit=MINUTES

# ---- Endpoint (cell) settings for the qos.transition-completed-topic.
#
qos.service.transition.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.transition.timeout.unit=MINUTES

# ---- Main external entry point for qos.
#
qos.service.requirements=${dcache.service.qos}
qos.service.requirements.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.requirements.timeout.unit=MINUTES

# ---- Internal endpoints consumed only by other qos services.
#
(immutable)qos.service.adjustment=${qos.adjuster.cell.name}
qos.service.adjustment.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.adjustment.timeout.unit=MINUTES

(immutable)qos.service.scanner=${qos.scanner.cell.name}
qos.service.scanner.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.scanner.timeout.unit=MINUTES

(immutable)qos.service.verification=${qos.verifier.cell.name}
qos.service.verification.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)qos.service.verification.timeout.unit=MINUTES
