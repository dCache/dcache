#  -----------------------------------------------------------------------
#     Default values for resilience
#
#     The resilience service is responsible for maintaining the requested
#     number of replicas for a given file when the file belongs to a
#     storage unit linked to a resilient pool group.  The actual requirements
#     are defined on the basis of the storage unit itself.
#
#  -----------------------------------------------------------------------
@DEFAULTS_HEADER@

#  ---- Cell name of resilience service
resilience.cell.name=Resilience

#  ---- Named queues to consume from
#
#   A service can consume messages from named queues. Other services can
#   write messages to such queues. A named queue has an unqualified cell
#   address, that is, an address without a domain name.
#
#   This property contains a comma separated list of named queues to
#   consume from.
#
resilience.cell.consume = ${resilience.cell.name}

# ---- Message topics to subscribe to.
#
resilience.cell.subscribe=${resilience.cache-location-topic},\
  ${resilience.qos-modification-topic},\
  ${resilience.corrupt-file-topic},\
  ${resilience.pool-monitor-topic}

# ---- Resilience listens for location updates from PnfsManager.
#
resilience.cache-location-topic=CacheLocationTopic

# ---- Resilience listens for attribute updates (for access latency and
#      retention policy) from PnfsManager.
#
resilience.qos-modification-topic=FileAttributesTopic

# ---- Listens for checksum scanner or pool reports.  If the corrupt
#      file is a non-unique replica, it tries to handle this by removing
#      the copy and making a new one.
#
resilience.corrupt-file-topic=CorruptFileTopic

# ---- Channel on which pool monitor updates are pushed out.
#      Resilience relies on these for current info regarding pools,
#      pool groups, storage units, pool mode/status, pool tags, and pool cost.
#
resilience.pool-monitor-topic=${dcache.pool-monitor.topic}

# ---- Configuration for database connection pool
#
#      The database connection pool reuses connections between successive
#      database operations.  By reusing connections dCache doesn't suffer
#      the overhead of establishing new database connections for each
#      operation.
#
#      The options here determine how resilience behaves as the
#      number of concurrent requests fluctuates.
# ------------------------------------------------------------------------

# ---- The maximum number of concurrent database connections.
#
#      The recommended minimum setting is:
#
#                  resilience.limits.file.submit-threads
#                  + resilience.limits.file.operation-threads
#                  + (2 * resilience.limits.pool.scan-threads)
#                  + a few extra for admin calls
#
#      Submit and operation threads require 1 database connection, and scan
#      threads need 2.
#
#      Since this service shares the chimera database with pnfsmanager,
#      be sure to adjust the postgresql.conf max connections upwards
#      to accommodate both.  Pnfsmanager runs well with about 100
#      connections.  Adding a separate resilience service means the
#      connections should be increased by at least the amount below.
#
resilience.db.connections.max = 120

# ---- The minimum number of idle database connections.
#
resilience.db.connections.idle = 1

(prefix)resilience.db.hikari-properties = Hikari-specific properties


# ---- Database related settings reserved for internal use.
#
(immutable)resilience.db.host=${chimera.db.host}
(immutable)resilience.db.name=${chimera.db.name}
(immutable)resilience.db.user=${chimera.db.user}
(immutable)resilience.db.password=${chimera.db.password}
(immutable)resilience.db.password.file=${chimera.db.password.file}
(immutable)resilience.db.url=${chimera.db.url}
(immutable)resilience.db.schema.changelog=${chimera.db.schema.changelog}
(immutable)resilience.db.schema.auto=false

# ---- Used with the pool scan query. This is a hint given to the jdbc driver
#      to decrease the number of round-trips to the database on large result
#      sets (by default it is 0, meaning ignored).  Setting this too high
#      may, however, adversely affect performance.
#
resilience.db.fetch-size=1000

#   -- replace with org.dcache.chimera.namespace.ChimeraEnstoreStorageInfoExtractor
#      if you are running an enstore HSM backend.
#
resilience.plugins.storage-info-extractor=${dcache.plugins.storage-info-extractor}

# ---- Base directory where any resilience metadata is stored.  This
#      includes the checkpoint file, inaccessible file lists, and statistics
#      output.
#
resilience.home=@dcache.paths.resilience@

# ---- Checkpointing.
#
#      How often the file operation table is to be saved to disk for
#      the purposes of recovery.
#
resilience.limits.checkpoint-expiry=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.limits.checkpoint-expiry.unit=MINUTES

# ---- Thread queues.
#
#       There are four thread queues associated with resilience handling
#           (see below). The first two are generally faster-running threads.
#           The latter two are slower, with (3) executing the most
#           time-consuming operation.
#
#       The settings here have been tuned to handle throughput on the order of
#           600+ new file messages a second.  In general it makes sense to give
#           more threads to the file copy task queue.
#           See further below for more details.
#

# ---- Thread queue used during the registration of a new operation.
#      Each thread makes a call to the database (requires a connection).
#
resilience.limits.file.submit-threads=70

# ---- Thread queue used when an operation becomes active (to verify
#      parameters and prepare a copy or remove task. Each thread makes
#      a call to the database (requires a connection).
#
resilience.limits.file.operation-threads=30

# ---- Thread queue used to do the actual file migration or removal.
#      There are no database calls on this thread.
#
#      A note on file operation throttling:
#
#      In order to guarantee that two operations on the same pnfsid are
#      not run concurrently, a queueing strategy is used.  This strategy also
#      throttles the total number of distinct file operations which can be
#      handled at a given time.  This number is determined from the file-copy
#      value.
#
resilience.limits.file.copy-threads=200

# ---- Thread queue used for scanning the namespace on pool state changes or
#      as part of a periodic check.  Requires a database connection,
#      which it holds onto for the life of the task being executed.
#      It also requires a second connection to check attributes.
#
#      A note on pool operation throttling:
#
#      A pool scan or processing of a pool status message can generate
#      thousands, even millions, of file tasks.  Allowing too many pool
#      operations to run simultaneously can, aside from the increased
#      pressure on the namespace database, potentially overload the system.
#      Lowering the number of available threads may be necessary
#      if the number of files per pool is on the order of 2 million or
#      greater (or, alternately, one may need to increase the memory of the
#      JVM for the resilience service).
#
#      A note on the memory footprint.  Each resilient file operation entry
#      requires 128 bytes minimum (assuming 8-byte alignment) of primitives or
#      object references.  4 of these references are to objects of variable
#      size on the heap which are usually shared with other structures.
#      If we assume each entry could encompass 512 bytes, then
#      10 million entries would require about 5 GB.  However, it is
#      advisable to set the resilience domain heap size to at least 8 GB,
#      because there is other overhead during normal processing which
#      can drive up peak usage values.  Naturally, if the service runs
#      together with other services in a JVM, the heap size should also be
#      adjusted upward.
#
resilience.limits.pool.scan-threads=5

# ---- Size of buffer for displaying history of the most
#      recently completed file operations.
#
resilience.limits.file.operation-history=1000

# ---- Operation queue fair-share management.
#
#      Resilience tries to apportion running tasks between 'foreground'
#      (incoming new locations) and 'background' (pnfsids from a scanned
#      pool) by weighting them according to the number of currently waiting
#      operations of each type.  To avoid starvation, however, a maximum limit
#      should be provided.  This is given as a percentage (0 < limit < 1).
#
resilience.limits.file.operation-max-allocation=0.8

# ---- Retry management.
#
#      The following property controls the number of
#      times the resilience system is allowed to retry failed file-operations.
#      This is on a per-source/target basis, if the error is judged retriable.
#      If there is a non-retriable error, but a different source or target
#      can be selected, the retry count is set back to 0 again.
#
resilience.limits.file.operation-retries=2

# ---- File operation checking.
#
#      The maximum interval which can pass before a check of waiting/completed
#      file operations is run (for an active system the interval will effectively
#      be shorter, as checks are also done each time a running task terminates).
#
resilience.limits.file.scan-period=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.limits.file.scan-period.unit=MINUTES

# ---- Pool manager pool info refreshing.
#
#      Information concerning pool cost is considered out of sync after
#      this interval has passed.   This should be somewhat longer than
#      the notification period value
#      (see poolmanager.pool-monitor.update-period and
#      poolmanager.pool-monitor.update-period.unit).
#
resilience.limits.pool-info-expiry=3
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.limits.pool-info-expiry.unit=MINUTES

# ---- Pool Status update handling.
#
#      How long to wait between the reception of a pool down update
#      and actually launching a scan operation to check replicas on
#      that pool.  Setting to 0 will trigger the scan immediately.
#
resilience.limits.pool.down-grace-period=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.limits.pool.down-grace-period.unit=HOURS

# ---- Pool Status update handling.
#
#      How long to wait between the reception of a pool restart update
#      and actually launching a scan operation to check replicas on
#      that pool. Setting to 0 will trigger the scan immediately.
#
resilience.limits.pool.restart-grace-period=6
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.limits.pool.restart-grace-period.unit=HOURS

# ---- Startup
#
#      When an entire dcache installation is brought on line at the same time,
#      pool status may not yet be available from the pool manager.  This
#      property sets an initial delay before resilience initialization
#      begins.  Setting this property to 0 skips the delay.
#
resilience.limits.startup-delay=30
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.limits.startup-delay.unit=SECONDS

# ---- Task launch
#
#      Scheduling the task can include a delay between when it has been selected to run
#      and when it actually starts to execute.  This may be useful in some situations
#      to avoid races between PnfsManager and resilience in terms of the intial
#      write to a pool.  This property can also be configured from the admin
#      interface using file ctrl reset.
#
resilience.limits.copy-launch-delay=0
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.limits.copy-launch-delay.unit=SECONDS

# ---- Copy/migration target selection.
#
#      Strategy implementation used to select among available/eligible
#      target pools.
#
resilience.pool-selection-strategy=org.dcache.pool.migration.ProportionalPoolSelectionStrategy

# ---- Periodic scanning (watchdog).
#
#      The following properties control the periodic scanning of the resilient
#      pools to check for replica consistency and initiate any copies or removes
#      that may be necessary in the case of inconsistent state.  The scan period
#      refers to the default amount of time between sweeps of the pools (absent
#      pool status change events). The scan window refers to the maximum amount
#      of time that can elapse since a pool was scanned after which another
#      scan of that specific pool will be initiated. Disabling the watchdog
#      means the system will only respond to actual pool state change events,
#      but will not automatically scan pools after the window period has elapsed.
#
(one-of?true|false)resilience.enable.watchdog=true
resilience.watchdog.scan.period=3
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.watchdog.scan.period.unit=MINUTES
resilience.watchdog.scan.window=24
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.watchdog.scan.window.unit=HOURS

# ---- Endpoint for contacting pin manager (passed on to migration task).
#
resilience.service.pinmanager=${dcache.service.pinmanager}

# ---- How long to wait for a response from the poolmanager.
#
resilience.service.pinmanager.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.service.pinmanager.timeout.unit=MINUTES

# ---- Endpoint for contacting pool manager (for staging requests).
#
resilience.service.poolmanager=${dcache.service.poolmanager}

# ---- How long to wait for a response from a pool.
#
resilience.service.pool.timeout=1
(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)resilience.service.pool.timeout.unit=MINUTES

# ---- Obsolete
#
(obsolete)resilience.cell.export = The resilience service is always exported
(obsolete)resilience.default-access-latency = value was not significant
(obsolete)resilience.default-retention-policy = value was not significant
(obsolete)resilience.enable.inherit-file-ownership = value was not significant
(obsolete)resilience.enable.full-path-permission-check = value was not significant
(obsolete)resilience.enable.acl = value was not significant
